{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75f63f75",
   "metadata": {},
   "source": [
    "# # ERA5 Enrichment for Telemetry data \n",
    " \n",
    "This notebook:\n",
    "1. Loads telemetry from parquet\n",
    "2. Determines the time range and spatial bounding boxes\n",
    "3. Downloads ERA5 wind & wave data per month (with tight monthly bboxes)\n",
    "4. Interpolates ERA5 data to the ship track (time + space, nearest)\n",
    "5. Writes monthly parquet files\n",
    "6. Merges all ERA5 fields back into the telemetry dataset and saves a final parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ab273b",
   "metadata": {},
   "source": [
    "##  --- 0. PREAMBLE & CONFIG ---\n",
    "- Set paths\n",
    "- Define ERA5 variables and dataset name\n",
    "- Tiny logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0f1904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from glob import glob\n",
    "import traceback\n",
    "import zipfile\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import cdsapi\n",
    "\n",
    "# Paths (edit if needed)\n",
    "TELEMETRY_PATH = \"./data/TraviataDataForTesting.parquet\"  # input telemetry\n",
    "OUT_DIR        = \"./metocean_out\"                         # monthly ERA5 outputs + final parquet\n",
    "ERA5_TMP_DIR   = \"./_era5_tmp\"                            # temporary ERA5 monthly files\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(ERA5_TMP_DIR, exist_ok=True)\n",
    "\n",
    "# ERA5 variables (wind + waves) from single levels\n",
    "ERA5_VARS = [\n",
    "    \"10m_u_component_of_wind\",\n",
    "    \"10m_v_component_of_wind\",\n",
    "    \"significant_height_of_combined_wind_waves_and_swell\",\n",
    "    \"mean_wave_period\",\n",
    "    \"mean_wave_direction\",\n",
    "]\n",
    "\n",
    "ERA5_DATASET = \"reanalysis-era5-single-levels\"\n",
    "\n",
    "\n",
    "def log(msg: str) -> None:\n",
    "    \"\"\"Simple timestamped logger.\"\"\"\n",
    "    print(time.strftime(\"%H:%M:%S\"), \"-\", msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de8d8e9",
   "metadata": {},
   "source": [
    "## --- 1. LOAD TELEMETRY & SCOPE ---\n",
    " - Load ship telemetry parquet\n",
    " - Ensure `timestamp` is tz-aware (UTC)\n",
    " - Add a `row_id` for stable merging\n",
    " - Build a minimal dataframe for ERA5 interpolation (`df_era`)\n",
    " - Compute overall time span\n",
    " - Define:\n",
    "   - a global bbox (for reference)\n",
    "   - a helper to compute monthly bboxes\n",
    "   - the list of months to process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c314d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load telemetry\n",
    "df = pd.read_parquet(TELEMETRY_PATH)\n",
    "\n",
    "# Ensure timestamp is UTC-aware\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True)\n",
    "\n",
    "# Stable key for merging\n",
    "df = df.reset_index(drop=True)\n",
    "df[\"row_id\"] = np.arange(len(df), dtype=np.int64)\n",
    "\n",
    "# Minimal subset for ERA5 interpolation\n",
    "cols_needed = [\"row_id\", \"timestamp\", \"Latitude_deg\", \"Longitude_deg\", \"HeadingTrue_deg\"]\n",
    "df_era = df[cols_needed].copy()\n",
    "\n",
    "# Time span\n",
    "t_min = df_era[\"timestamp\"].min()\n",
    "t_max = df_era[\"timestamp\"].max()\n",
    "log(f\"time span: {t_min} → {t_max}\")\n",
    "\n",
    "\n",
    "def bbox_quantile(d: pd.DataFrame, qlow=0.01, qhigh=0.99, pad=2.0):\n",
    "    \"\"\"\n",
    "    Global bounding box using quantiles to avoid outliers, padded by `pad` degrees.\n",
    "    Returns [N, W, S, E] for CDS.\n",
    "    \"\"\"\n",
    "    lat_min = float(d[\"Latitude_deg\"].quantile(qlow))\n",
    "    lat_max = float(d[\"Latitude_deg\"].quantile(qhigh))\n",
    "    lon_min = float(d[\"Longitude_deg\"].quantile(qlow))\n",
    "    lon_max = float(d[\"Longitude_deg\"].quantile(qhigh))\n",
    "\n",
    "    N = min(lat_max + pad, 90.0)\n",
    "    S = max(lat_min - pad, -90.0)\n",
    "    W = max(lon_min - pad, -180.0)\n",
    "    E = min(lon_max + pad, 180.0)\n",
    "    return [N, W, S, E]\n",
    "\n",
    "\n",
    "def month_bbox(df_m: pd.DataFrame, pad=2.0):\n",
    "    \"\"\"\n",
    "    Compute a tight bounding box for a single month's telemetry,\n",
    "    then pad it by `pad` degrees. Returns [N, W, S, E].\n",
    "    \"\"\"\n",
    "    lat_min = float(df_m[\"Latitude_deg\"].min())\n",
    "    lat_max = float(df_m[\"Latitude_deg\"].max())\n",
    "    lon_min = float(df_m[\"Longitude_deg\"].min())\n",
    "    lon_max = float(df_m[\"Longitude_deg\"].max())\n",
    "\n",
    "    N = min(lat_max + pad, 90.0)\n",
    "    S = max(lat_min - pad, -90.0)\n",
    "    W = max(lon_min - pad, -180.0)\n",
    "    E = min(lon_max + pad, 180.0)\n",
    "    return [N, W, S, E]\n",
    "\n",
    "\n",
    "def month_list(t0, t1):\n",
    "    \"\"\"Return list of YYYY-MM strings from t0 to t1 inclusive.\"\"\"\n",
    "    cur = pd.Timestamp(t0).to_period(\"M\")\n",
    "    last = pd.Timestamp(t1).to_period(\"M\")\n",
    "    out = []\n",
    "    while cur <= last:\n",
    "        out.append(str(cur))\n",
    "        cur = cur + 1\n",
    "    return out\n",
    "\n",
    "\n",
    "# Global bbox (mainly informative)\n",
    "area_global = bbox_quantile(df_era, 0.01, 0.99, 2.0)\n",
    "log(f\"Global ERA5 bbox [N,W,S,E]: {area_global}\")\n",
    "\n",
    "# List of months to process\n",
    "months = month_list(t_min, t_max)\n",
    "log(f\"months to fetch: {months[:3]} ... {months[-3:]}  (total {len(months)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5bef44",
   "metadata": {},
   "source": [
    "## --- 2. ERA5 ONE-MONTH DOWNLOADER ---\n",
    "- Use CDS API to download one month\n",
    "- We request NetCDF, but CDS may return ZIP → detect and rename\n",
    "- Function returns path to either `.nc` or `.zip` (caller handles both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2ec56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def era5_download_month(year_month: str, area, variables, out_path_nc):\n",
    "    \"\"\"\n",
    "    Download one month of ERA5 single-level data.\n",
    "    If CDS returns a ZIP, rename it to .zip and return that path.\n",
    "    Otherwise return the .nc path.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    year_month : str\n",
    "        'YYYY-MM'\n",
    "    area : list[float]\n",
    "        [N, W, S, E] bounding box\n",
    "    variables : list[str]\n",
    "        ERA5 variable names\n",
    "    out_path_nc : str\n",
    "        Target path for NetCDF (we may rename to .zip)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Path to downloaded .nc or .zip file.\n",
    "    \"\"\"\n",
    "    year, month = year_month.split(\"-\")\n",
    "    days = [f\"{d:02d}\" for d in range(1, 32)]\n",
    "    hours = [f\"{h:02d}:00\" for h in range(24)]\n",
    "\n",
    "    out_base = os.path.splitext(out_path_nc)[0]\n",
    "    out_path_zip = out_base + \".zip\"\n",
    "\n",
    "    # Skip if already present\n",
    "    if os.path.isfile(out_path_nc):\n",
    "        log(f\"[skip] exists: {out_path_nc}\")\n",
    "        return out_path_nc\n",
    "    if os.path.isfile(out_path_zip):\n",
    "        log(f\"[skip] exists: {out_path_zip}\")\n",
    "        return out_path_zip\n",
    "\n",
    "    req = {\n",
    "        \"product_type\": \"reanalysis\",\n",
    "        \"variable\": variables,\n",
    "        \"year\": year,\n",
    "        \"month\": month,\n",
    "        \"day\": days,\n",
    "        \"time\": hours,\n",
    "        \"data_format\": \"netcdf\",\n",
    "        \"download_format\": \"unarchived\",  # sometimes ignored by CDS\n",
    "        \"area\": area,  # [N, W, S, E]\n",
    "    }\n",
    "\n",
    "    tmp_path = out_path_nc\n",
    "    log(f\"[dl] {year_month} → {tmp_path}\")\n",
    "    c = cdsapi.Client()\n",
    "    c.retrieve(ERA5_DATASET, req, tmp_path)\n",
    "\n",
    "    # Detect if CDS actually gave us a zip\n",
    "    if zipfile.is_zipfile(tmp_path):\n",
    "        new_path = out_path_zip\n",
    "        os.rename(tmp_path, new_path)\n",
    "        log(f\"[note] CDS returned ZIP → renamed to {new_path}\")\n",
    "        return new_path\n",
    "    else:\n",
    "        return tmp_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97270c7",
   "metadata": {},
   "source": [
    "## --- 3. OPEN & INTERPOLATE ONE MONTH ---\n",
    "\n",
    " - Robust NetCDF opener\n",
    " - Standardize ERA5 dataset (rename variables, resolve dims, sort)\n",
    " - Longitude wrapping to [0, 360)\n",
    " - Chunked interpolation:\n",
    "   - Nearest in time (with tolerance)\n",
    "   - Nearest in space\n",
    " - Support for both:\n",
    "   - Plain `.nc` files\n",
    "   - ZIP files containing one or more `.nc` members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e410cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_nc(path: str) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Open a NetCDF file with a sequence of engines for robustness.\n",
    "    \"\"\"\n",
    "    engines = (\"netcdf4\", \"h5netcdf\", \"scipy\")\n",
    "    for eng in engines:\n",
    "        try:\n",
    "            return xr.open_dataset(path, engine=eng)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Last resort (let xarray decide)\n",
    "    return xr.open_dataset(path)\n",
    "\n",
    "\n",
    "def standardize_ds(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Make ERA5 consistent for interpolation:\n",
    "    - valid_time -> time\n",
    "    - pick one ensemble member (if present)\n",
    "    - collapse expver dimension\n",
    "    - sort coordinates\n",
    "    - rename key variables to short names\n",
    "    \"\"\"\n",
    "    if \"valid_time\" in ds.coords and \"time\" not in ds.coords:\n",
    "        ds = ds.rename({\"valid_time\": \"time\"})\n",
    "    if \"number\" in ds.dims:\n",
    "        ds = ds.isel(number=0, drop=True)\n",
    "    if \"expver\" in ds.dims:\n",
    "        ds = ds.max(\"expver\", skipna=True, keep_attrs=True)\n",
    "\n",
    "    for c in (\"time\", \"latitude\", \"longitude\"):\n",
    "        if c in ds.coords:\n",
    "            ds = ds.sortby(c)\n",
    "\n",
    "    rename = {\n",
    "        \"10m_u_component_of_wind\": \"u10\",\n",
    "        \"10m_v_component_of_wind\": \"v10\",\n",
    "        \"significant_height_of_combined_wind_waves_and_swell\": \"swh\",\n",
    "        \"mean_wave_period\": \"mwp\",\n",
    "        \"mean_wave_direction\": \"mwd\",\n",
    "    }\n",
    "    ds = ds.rename({k: v for k, v in rename.items() if k in ds.data_vars})\n",
    "    return ds\n",
    "\n",
    "\n",
    "def wrap_lon_to(ds: xr.Dataset, mode: str = \"360\") -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Ensure dataset longitudes match telemetry convention.\n",
    "\n",
    "    mode=\"360\"  → [0, 360)\n",
    "    mode=\"-180\" → [-180, 180)\n",
    "    \"\"\"\n",
    "    if \"longitude\" not in ds.coords:\n",
    "        return ds\n",
    "\n",
    "    lon = ds.longitude\n",
    "    if mode == \"360\":\n",
    "        if float(lon.max()) <= 180.0:\n",
    "            ds = ds.assign_coords(longitude=((lon + 360) % 360)).sortby(\"longitude\")\n",
    "    else:\n",
    "        if float(lon.max()) > 180.0:\n",
    "            ds = ds.assign_coords(longitude=((lon + 180) % 360) - 180).sortby(\"longitude\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "def interp_time_then_space_chunked_nearest(\n",
    "    ds: xr.Dataset,\n",
    "    df_m: pd.DataFrame,\n",
    "    ym: str,\n",
    "    time_tol=\"90min\",\n",
    "    batch_size=10_000,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Chunked + NEAREST interpolation of ERA5 onto ship track for a single month.\n",
    "\n",
    "    Steps:\n",
    "    - For each batch of telemetry rows:\n",
    "      1) Nearest-in-time (with tolerance)\n",
    "      2) Nearest in space (lat/lon)\n",
    "    - Return a DataFrame indexed by row_id with interpolated variables.\n",
    "    \"\"\"\n",
    "    t_all   = df_m[\"ts_naive\"].to_numpy().astype(\"datetime64[ns]\")\n",
    "    lon_all = ((df_m[\"Longitude_deg\"].to_numpy()) + 360.0) % 360.0\n",
    "    lat_all = df_m[\"Latitude_deg\"].to_numpy()\n",
    "    row_all = df_m[\"row_id\"].to_numpy()\n",
    "\n",
    "    n = len(df_m)\n",
    "    vars_ = [v for v in [\"u10\", \"v10\", \"swh\", \"mwp\", \"mwd\"] if v in ds.data_vars]\n",
    "\n",
    "    out_chunks = []\n",
    "    tol = np.timedelta64(pd.Timedelta(time_tol))\n",
    "\n",
    "    for start in range(0, n, batch_size):\n",
    "        end = min(start + batch_size, n)\n",
    "        log(f\"[{ym}] interp batch {start}:{end} of {n}\")\n",
    "\n",
    "        t_chunk   = t_all[start:end]\n",
    "        lon_chunk = lon_all[start:end]\n",
    "        lat_chunk = lat_all[start:end]\n",
    "        row_chunk = row_all[start:end]\n",
    "\n",
    "        # Nearest in time (with tolerance)\n",
    "        ds_t = ds.sel(\n",
    "            time=xr.DataArray(t_chunk, dims=\"p\"),\n",
    "            method=\"nearest\",\n",
    "            tolerance=tol,\n",
    "        )\n",
    "\n",
    "        # Nearest in space\n",
    "        ds_nearest = ds_t.interp(\n",
    "            longitude=(\"p\", lon_chunk),\n",
    "            latitude=(\"p\", lat_chunk),\n",
    "            method=\"nearest\",\n",
    "        )\n",
    "\n",
    "        df_out_batch = pd.DataFrame(index=row_chunk)\n",
    "        for v in vars_:\n",
    "            df_out_batch[v] = ds_nearest[v].values\n",
    "\n",
    "        df_out_batch.index.name = \"row_id\"\n",
    "        out_chunks.append(df_out_batch)\n",
    "\n",
    "    out = pd.concat(out_chunks).sort_index()\n",
    "    return out\n",
    "\n",
    "\n",
    "def interpolate_zip_by_member_robust(zip_path: str, df_m: pd.DataFrame, ym: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Open a CDS ZIP, interpolate EACH .nc member separately (on its native grid)\n",
    "    using chunked nearest interpolation, then combine on row_id.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    with zipfile.ZipFile(zip_path) as z:\n",
    "        members = [m for m in z.namelist() if m.endswith(\".nc\")]\n",
    "        if not members:\n",
    "            raise FileNotFoundError(f\"No .nc inside zip: {zip_path}\")\n",
    "\n",
    "        for m in members:\n",
    "            with tempfile.NamedTemporaryFile(suffix=\".nc\", delete=False) as tmp:\n",
    "                tmp.write(z.read(m))\n",
    "                one_nc_path = tmp.name\n",
    "\n",
    "            try:\n",
    "                ds_one = open_nc(one_nc_path)\n",
    "                ds_one = standardize_ds(ds_one)\n",
    "                ds_one = wrap_lon_to(ds_one, \"360\")\n",
    "\n",
    "                has = [v for v in [\"u10\", \"v10\", \"swh\", \"mwp\", \"mwd\"] if v in ds_one.data_vars]\n",
    "                log(f\"[member] {ym}:{m} provides: {has}\")\n",
    "\n",
    "                df_chunk = interp_time_then_space_chunked_nearest(\n",
    "                    ds_one, df_m, ym, time_tol=\"90min\", batch_size=10_000\n",
    "                )\n",
    "                if not df_chunk.empty:\n",
    "                    results.append(df_chunk)\n",
    "\n",
    "            finally:\n",
    "                try:\n",
    "                    os.remove(one_nc_path)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    if not results:\n",
    "        raise ValueError(f\"{ym}: no variables were interpolated from ZIP members.\")\n",
    "    out = results[0]\n",
    "    for r in results[1:]:\n",
    "        out = out.combine_first(r)\n",
    "    return out\n",
    "\n",
    "\n",
    "def interpolate_one_file_robust(ds: xr.Dataset, df_m: pd.DataFrame, ym: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robust interpolation for a single plain .nc using the same\n",
    "    chunked + nearest strategy.\n",
    "    \"\"\"\n",
    "    ds = standardize_ds(ds)\n",
    "    ds = wrap_lon_to(ds, \"360\")\n",
    "    if \"time\" not in ds.coords:\n",
    "        raise ValueError(f\"{ym}: dataset has no 'time' coordinate after standardize_ds.\")\n",
    "\n",
    "    return interp_time_then_space_chunked_nearest(\n",
    "        ds, df_m, ym, time_tol=\"90min\", batch_size=10_000\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c425809",
   "metadata": {},
   "source": [
    "## --- 4. STREAM ALL MONTHS: DOWNLOAD → INTERP → WRITE → CLEAN ---\n",
    " - Loop over months\n",
    " - Compute a **tight bbox per month**\n",
    " - Download ERA5 (ZIP or NC)\n",
    " - Interpolate onto telemetry rows\n",
    " - Write one parquet per month: `era5_interp_YYYY-MM.parquet`\n",
    " - Optionally skip months that already have output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710ae19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_months(df_all: pd.DataFrame, months, variables, out_dir, tmp_dir):\n",
    "    \"\"\"\n",
    "    For each month in `months`:\n",
    "      - Subset telemetry for that month\n",
    "      - Compute month-specific bounding box\n",
    "      - Download ERA5 (nc or zip)\n",
    "      - Interpolate onto ship track\n",
    "      - Write monthly parquet\n",
    "    \"\"\"\n",
    "    df_all = df_all.copy()\n",
    "\n",
    "    # tz-naive helper for xarray\n",
    "    if \"ts_naive\" not in df_all.columns:\n",
    "        df_all[\"ts_naive\"] = df_all[\"timestamp\"].dt.tz_convert(\"UTC\").dt.tz_localize(None)\n",
    "\n",
    "    out_parts = []\n",
    "\n",
    "    for ym in months:\n",
    "        ds = None\n",
    "        file_path = None\n",
    "        try:\n",
    "            # rows in this month\n",
    "            mask = df_all[\"ts_naive\"].dt.to_period(\"M\") == pd.Period(ym)\n",
    "            df_m = df_all.loc[\n",
    "                mask,\n",
    "                [\"row_id\", \"timestamp\", \"ts_naive\", \"Latitude_deg\", \"Longitude_deg\"],\n",
    "            ].copy()\n",
    "\n",
    "            if df_m.empty:\n",
    "                log(f\"[skip ] {ym}: no rows in this month\")\n",
    "                continue\n",
    "\n",
    "            # Month-specific bbox\n",
    "            bbox_m = month_bbox(df_m, pad=2.0)\n",
    "            log(f\"[bbox ] {ym}: {bbox_m}\")\n",
    "\n",
    "            # Download month (nc or zip)\n",
    "            target_nc = os.path.join(tmp_dir, f\"ERA5_{ym}.nc\")\n",
    "            file_path = era5_download_month(ym, bbox_m, variables, target_nc)\n",
    "            log(f\"[open ] {ym}: {file_path}\")\n",
    "\n",
    "            # Interpolate\n",
    "            if zipfile.is_zipfile(file_path):\n",
    "                df_w = interpolate_zip_by_member_robust(file_path, df_m, ym)\n",
    "            else:\n",
    "                ds = open_nc(file_path)\n",
    "                try:\n",
    "                    log(f\"[info ] {ym}: vars={list(ds.data_vars)[:8]}...\")\n",
    "                    log(f\"[info ] {ym}: coords={list(ds.coords)} | dims={dict(ds.dims)}\")\n",
    "                except Exception:\n",
    "                    pass\n",
    "                log(f\"[interp] {ym} for {len(df_m)} rows …\")\n",
    "                df_w = interpolate_one_file_robust(ds, df_m, ym)\n",
    "\n",
    "            # Write monthly parquet\n",
    "            part_path = os.path.join(out_dir, f\"era5_interp_{ym}.parquet\")\n",
    "            df_w.to_parquet(part_path)\n",
    "            out_parts.append(part_path)\n",
    "            log(f\"[done ] {ym}: wrote {part_path} (cols={list(df_w.columns)})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            log(f\"[ERROR] {ym}: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "        finally:\n",
    "            try:\n",
    "                if ds is not None:\n",
    "                    ds.close()\n",
    "            except Exception:\n",
    "                pass\n",
    "            # If you want to clean the raw ERA5 file, you can uncomment:\n",
    "            # if file_path and os.path.isfile(file_path):\n",
    "            #     os.remove(file_path)\n",
    "            #     log(f\"[clean] removed {file_path}\")\n",
    "\n",
    "    return out_parts\n",
    "\n",
    "\n",
    "# Skip months we've already processed\n",
    "_done = {\n",
    "    os.path.basename(p)[len(\"era5_interp_\"):-len(\".parquet\")]\n",
    "    for p in glob(os.path.join(OUT_DIR, \"era5_interp_*.parquet\"))\n",
    "}\n",
    "months_pending = [m for m in months if m not in _done]\n",
    "log(f\"Remaining months: {months_pending}\")\n",
    "\n",
    "parts = process_all_months(\n",
    "    df_all   = df_era,\n",
    "    months   = months_pending,\n",
    "    variables= ERA5_VARS,\n",
    "    out_dir  = OUT_DIR,\n",
    "    tmp_dir  = ERA5_TMP_DIR,\n",
    ")\n",
    "\n",
    "log(f\"wrote {len(parts)} newly processed monthly parquet parts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307e1b8a",
   "metadata": {},
   "source": [
    "##  5. Coverage Report per Month (Optional QA)\n",
    " \n",
    " - For each monthly parquet, print:\n",
    "   - number of rows\n",
    "   - NaN coverage for `u10`, `v10`, `swh`, `mwp`, `mwd`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d551a381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage_report(df_like: pd.DataFrame):\n",
    "    for v in [\"u10\", \"v10\", \"swh\", \"mwp\", \"mwd\"]:\n",
    "        if v in df_like.columns:\n",
    "            n = len(df_like)\n",
    "            k = df_like[v].isna().sum()\n",
    "            print(f\"    {v}: {k}/{n} = {k/n:.2%} NaN\")\n",
    "    print()\n",
    "\n",
    "\n",
    "parqs = sorted(glob(os.path.join(OUT_DIR, \"era5_interp_*.parquet\")))\n",
    "if not parqs:\n",
    "    raise FileNotFoundError(\"No monthly parquet files found in OUT_DIR.\")\n",
    "\n",
    "print(f\"Found {len(parqs)} monthly files\\n\")\n",
    "for part_path in parqs:\n",
    "    ym = os.path.basename(part_path)[len(\"era5_interp_\"):-len(\".parquet\")]\n",
    "    w = pd.read_parquet(part_path)\n",
    "    print(f\"=== {ym} ===\")\n",
    "    print(f\"Rows: {len(w)}\")\n",
    "    coverage_report(w)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f986dd",
   "metadata": {},
   "source": [
    "##  6. Merge All ERA5 Parts Back onto Telemetry and Save\n",
    " \n",
    " - Concatenate all monthly ERA5 parquet files\n",
    " - Merge on `row_id` with the original telemetry\n",
    " - Quick NaN check on `u10`/`v10`\n",
    " - Save final enriched dataset as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9664b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_files = sorted(glob(os.path.join(OUT_DIR, \"era5_interp_*.parquet\")))\n",
    "weather_all = pd.concat([pd.read_parquet(p) for p in part_files]).sort_index()\n",
    "log(f\"weather rows: {len(weather_all)} | columns: {list(weather_all.columns)}\")\n",
    "\n",
    "# Left-join on row_id\n",
    "df_merged = df.merge(weather_all, on=\"row_id\", how=\"left\")\n",
    "\n",
    "# Quick quality check\n",
    "nan_frac = df_merged[[\"u10\", \"v10\"]].isna().any(axis=1).mean()\n",
    "log(f\"NaN fraction in ERA5 wind columns: {nan_frac:.2%}\")\n",
    "\n",
    "# Save final enriched dataset\n",
    "FINAL_OUT = os.path.join(OUT_DIR, \"telemetry_with_era5_wind_waves.parquet\")\n",
    "df_merged.to_parquet(FINAL_OUT, index=False)\n",
    "log(f\"saved: {FINAL_OUT}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "era5-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
