{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0ead2e1",
   "metadata": {},
   "source": [
    "##  Ocean Enrichment for Telemetry + ERA5\n",
    " \n",
    " This notebook:\n",
    " 1. Loads telemetry already enriched with ERA5\n",
    " 2. Determines time span and monthly bounding boxes\n",
    " 3. Downloads Copernicus Marine ocean data (uo, vo, thetao, so) per month\n",
    " 4. Interpolates ocean data to the ship track (time + space)\n",
    " 5. Writes monthly parquet files\n",
    " 6. Merges all ocean fields back into the telemetry+ERA5 dataset\n",
    " 7. Computes surface density from thetao & salinity and saves a final parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50033bd",
   "metadata": {},
   "source": [
    "##  --- 0. PREAMBLE & CONFIG ---\n",
    " - Set paths\n",
    " - Define Copernicus Marine dataset and variables\n",
    " - Simple logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bd03f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "from copernicusmarine import subset as cm_subset  # Copernicus Marine subset API\n",
    "\n",
    "# Paths\n",
    "TELEMETRY_PATH = \"metocean_out/telemetry_with_era5_wind_waves.parquet\"\n",
    "\n",
    "OUT_DIR_OCEAN  = \"./test\"\n",
    "OCEAN_TMP_DIR  = \"./_ocean_tmp\"\n",
    "os.makedirs(OUT_DIR_OCEAN, exist_ok=True)\n",
    "os.makedirs(OCEAN_TMP_DIR, exist_ok=True)\n",
    "\n",
    "# Copernicus Marine dataset + variables\n",
    "OCEAN_DATASET_ID = \"cmems_mod_glo_phy_anfc_0.083deg_PT1H-m\"  # hourly global physics\n",
    "OCEAN_VARS = [\"uo\", \"vo\", \"thetao\", \"so\"]  # surface u, v, potential T, salinity\n",
    "\n",
    "\n",
    "def log(msg: str) -> None:\n",
    "    \"\"\"Simple timestamped logger.\"\"\"\n",
    "    print(time.strftime(\"%H:%M:%S\"), \"-\", msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd8a827",
   "metadata": {},
   "source": [
    "##  --- 1. LOAD TELEMETRY & SCOPE ---\n",
    " - Load telemetry+ERA5 parquet\n",
    " - Ensure timestamp is UTC-aware\n",
    " - Add `row_id` as stable key\n",
    " - Build minimal DF for ocean interpolation\n",
    " - Compute overall time span\n",
    " - Define global bbox (info only) and monthly bbox helper\n",
    " - Build list of months to process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3d9b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(TELEMETRY_PATH)\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True)\n",
    "df = df.reset_index(drop=True)\n",
    "df[\"row_id\"] = np.arange(len(df), dtype=np.int64)\n",
    "\n",
    "cols_needed = [\"row_id\", \"timestamp\", \"Latitude_deg\", \"Longitude_deg\"]\n",
    "df_ocean = df[cols_needed].copy()\n",
    "\n",
    "t_min = df_ocean[\"timestamp\"].min()\n",
    "t_max = df_ocean[\"timestamp\"].max()\n",
    "log(f\"time span: {t_min} → {t_max}\")\n",
    "\n",
    "\n",
    "def bbox_quantile(d: pd.DataFrame, qlow=0.01, qhigh=0.99, pad=2.0):\n",
    "    \"\"\"\n",
    "    Global bounding box using quantiles to avoid outliers, padded by `pad` degrees.\n",
    "    Returns [N, W, S, E].\n",
    "    \"\"\"\n",
    "    lat_min = float(d[\"Latitude_deg\"].quantile(qlow))\n",
    "    lat_max = float(d[\"Latitude_deg\"].quantile(qhigh))\n",
    "    lon_min = float(d[\"Longitude_deg\"].quantile(qlow))\n",
    "    lon_max = float(d[\"Longitude_deg\"].quantile(qhigh))\n",
    "\n",
    "    N = min(lat_max + pad, 90.0)\n",
    "    S = max(lat_min - pad, -90.0)\n",
    "    W = max(lon_min - pad, -180.0)\n",
    "    E = min(lon_max + pad, 180.0)\n",
    "    return [N, W, S, E]\n",
    "\n",
    "\n",
    "def month_bbox(df_m: pd.DataFrame, pad=2.0):\n",
    "    \"\"\"\n",
    "    Compute a tight bounding box for a single month of telemetry,\n",
    "    then pad it by `pad` degrees in all directions.\n",
    "    Returns [N, W, S, E] for Copernicus.\n",
    "    \"\"\"\n",
    "    lat_min = float(df_m[\"Latitude_deg\"].min())\n",
    "    lat_max = float(df_m[\"Latitude_deg\"].max())\n",
    "    lon_min = float(df_m[\"Longitude_deg\"].min())\n",
    "    lon_max = float(df_m[\"Longitude_deg\"].max())\n",
    "\n",
    "    N = min(lat_max + pad, 90.0)\n",
    "    S = max(lat_min - pad, -90.0)\n",
    "    W = max(lon_min - pad, -180.0)\n",
    "    E = min(lon_max + pad, 180.0)\n",
    "    return [N, W, S, E]  # [north, west, south, east]\n",
    "\n",
    "\n",
    "def month_list(t0, t1):\n",
    "    \"\"\"Return list of YYYY-MM strings from t0 to t1 inclusive.\"\"\"\n",
    "    cur = pd.Timestamp(t0).to_period(\"M\")\n",
    "    last = pd.Timestamp(t1).to_period(\"M\")\n",
    "    out = []\n",
    "    while cur <= last:\n",
    "        out.append(str(cur))\n",
    "        cur = cur + 1\n",
    "    return out\n",
    "\n",
    "\n",
    "# Global bbox (for reference)\n",
    "area_global = bbox_quantile(df_ocean, 0.01, 0.99, 2.0)\n",
    "log(f\"OCEAN bbox [N,W,S,E]: {area_global}\")\n",
    "\n",
    "# Months to process\n",
    "months = month_list(t_min, t_max)\n",
    "log(f\"months to fetch: {months[:3]} ... {months[-3:]} (total {len(months)})\")\n",
    "\n",
    "# tz-naive helper for interpolation (xarray)\n",
    "if \"ts_naive\" not in df_ocean.columns:\n",
    "    df_ocean = df_ocean.copy()\n",
    "    df_ocean[\"ts_naive\"] = df_ocean[\"timestamp\"].dt.tz_convert(\"UTC\").dt.tz_localize(None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be0e3ed",
   "metadata": {},
   "source": [
    "##  --- 2. DOWNLOAD ONE MONTH OF OCEAN DATA (Copernicus Marine) ---\n",
    " - Use Copernicus Marine `subset` API\n",
    " - Monthly time window\n",
    " - Monthly bbox\n",
    " - Save to NetCDF per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c206f64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocean_download_month(year_month: str, area, vars_list, out_nc_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Download one month of ocean data via Copernicus Marine `subset` API.\n",
    "    `area` is [N, W, S, E] for THIS MONTH ONLY.\n",
    "    \"\"\"\n",
    "    if os.path.isfile(out_nc_path):\n",
    "        log(f\"[skip] ocean exists: {out_nc_path}\")\n",
    "        return out_nc_path\n",
    "\n",
    "    year, month = map(int, year_month.split(\"-\"))\n",
    "\n",
    "    # Monthly time span [t0, t1)\n",
    "    t0 = pd.Timestamp(year=year, month=month, day=1, tz=\"UTC\")\n",
    "    if month == 12:\n",
    "        t1 = pd.Timestamp(year=year + 1, month=1, day=1, tz=\"UTC\")\n",
    "    else:\n",
    "        t1 = pd.Timestamp(year=year, month=month + 1, day=1, tz=\"UTC\")\n",
    "\n",
    "    north, west, south, east = area  # [N, W, S, E]\n",
    "\n",
    "    log(f\"[dl-ocean] {year_month} bbox={area} → {out_nc_path}\")\n",
    "    cm_subset(\n",
    "        dataset_id=OCEAN_DATASET_ID,\n",
    "        variables=vars_list,\n",
    "        start_datetime=t0.isoformat(),\n",
    "        end_datetime=t1.isoformat(),\n",
    "        minimum_latitude=south,\n",
    "        maximum_latitude=north,\n",
    "        minimum_longitude=west,\n",
    "        maximum_longitude=east,\n",
    "        output_filename=out_nc_path,\n",
    "        overwrite=False,\n",
    "    )\n",
    "    return out_nc_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca51bae",
   "metadata": {},
   "source": [
    "##  --- 3. OPEN & INTERPOLATE ONE MONTH TO THE SHIP TRACK ---\n",
    " - Open monthly NetCDF with chunking\n",
    " - Keep only surface layer (depth=0) if depth exists\n",
    " - Wrap longitudes to [0, 360)\n",
    " - Map each ship time to nearest model time (with tolerance)\n",
    " - Interpolate spatially (linear + nearest fallback) onto ship track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2947ceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_ocean_nc(path: str) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Open NetCDF with chunking and keep only surface (depth=0) if depth exists.\n",
    "    Remove depth dimension/coordinate when present.\n",
    "    \"\"\"\n",
    "    ds = xr.open_dataset(\n",
    "        path,\n",
    "        chunks={\"time\": 24, \"latitude\": 200, \"longitude\": 200},\n",
    "    )\n",
    "\n",
    "    # Case A – depth is a full dimension: pick surface & drop dim\n",
    "    if \"depth\" in ds.dims:\n",
    "        ds = ds.isel(depth=0, drop=True)\n",
    "    # Case B – depth is only a coordinate\n",
    "    elif \"depth\" in ds.coords:\n",
    "        ds = ds.drop_vars(\"depth\", errors=\"ignore\")\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "def wrap_lon_360(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Ensure longitude in [0, 360), sorted.\n",
    "    \"\"\"\n",
    "    if \"longitude\" not in ds.coords:\n",
    "        return ds\n",
    "    lon = ds[\"longitude\"]\n",
    "    if float(lon.max()) <= 180.0:\n",
    "        ds = ds.assign_coords(longitude=((lon + 360) % 360)).sortby(\"longitude\")\n",
    "    return ds\n",
    "\n",
    "\n",
    "def ocean_interp_time_space(ds: xr.Dataset, df_m: pd.DataFrame, time_tol: str = \"90min\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Interpolate ocean ds (uo, vo, thetao, so) to ship positions+times.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ds : xarray.Dataset\n",
    "        Must have dims (time, latitude, longitude)\n",
    "    df_m : pandas.DataFrame\n",
    "        With columns: ['row_id', 'timestamp', 'ts_naive', 'Latitude_deg', 'Longitude_deg']\n",
    "    time_tol : str\n",
    "        Max allowed time distance between ship time and model time (e.g. '90min').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Index=row_id, columns=subset of [uo, vo, thetao, so]\n",
    "    \"\"\"\n",
    "    ds = wrap_lon_360(ds)\n",
    "    if \"time\" not in ds.coords:\n",
    "        raise ValueError(\"Ocean ds has no 'time' coord.\")\n",
    "\n",
    "    # 1) Map each ship timestamp to nearest model time index\n",
    "    t_mod = ds[\"time\"].values  # model times\n",
    "    t_req = df_m[\"ts_naive\"].to_numpy().astype(\"datetime64[ns]\")\n",
    "\n",
    "    t_mod_i64 = t_mod.astype(\"datetime64[ns]\").astype(\"int64\")\n",
    "    t_req_i64 = t_req.astype(\"int64\")\n",
    "\n",
    "    idx_right = np.searchsorted(t_mod_i64, t_req_i64)\n",
    "    idx_left  = np.clip(idx_right - 1, 0, len(t_mod_i64) - 1)\n",
    "    idx_right = np.clip(idx_right,      0, len(t_mod_i64) - 1)\n",
    "\n",
    "    diff_left  = np.abs(t_mod_i64[idx_left]  - t_req_i64)\n",
    "    diff_right = np.abs(t_mod_i64[idx_right] - t_req_i64)\n",
    "    nearest_idx = np.where(diff_right < diff_left, idx_right, idx_left)\n",
    "\n",
    "    # Apply time tolerance\n",
    "    tol_ns = int(pd.Timedelta(time_tol).to_numpy())  # nanoseconds\n",
    "    diff_ns = np.abs(t_mod_i64[nearest_idx] - t_req_i64)\n",
    "    valid = diff_ns <= tol_ns\n",
    "\n",
    "    df_valid = df_m.loc[valid].copy()\n",
    "    df_valid[\"time_idx\"] = nearest_idx[valid]\n",
    "\n",
    "    # 2) Output frame\n",
    "    vars_ = [v for v in [\"uo\", \"vo\", \"thetao\", \"so\"] if v in ds.data_vars]\n",
    "    out = pd.DataFrame(index=df_m[\"row_id\"].values, columns=vars_, dtype=\"float32\")\n",
    "\n",
    "    # 3) Loop per unique model time index\n",
    "    for ti in np.unique(df_valid[\"time_idx\"].values):\n",
    "        df_chunk = df_valid[df_valid[\"time_idx\"] == ti]\n",
    "        if df_chunk.empty:\n",
    "            continue\n",
    "\n",
    "        t_val = t_mod[ti]\n",
    "        ds_slice = ds.sel(time=t_val)\n",
    "\n",
    "        lon360 = ((df_chunk[\"Longitude_deg\"].to_numpy()) + 360.0) % 360.0\n",
    "        lat    = df_chunk[\"Latitude_deg\"].to_numpy()\n",
    "\n",
    "        try:\n",
    "            primary = ds_slice.interp(\n",
    "                longitude=(\"p\", lon360),\n",
    "                latitude=(\"p\", lat),\n",
    "                method=\"linear\",\n",
    "            )\n",
    "            nearest = ds_slice.interp(\n",
    "                longitude=(\"p\", lon360),\n",
    "                latitude=(\"p\", lat),\n",
    "                method=\"nearest\",\n",
    "            )\n",
    "        except Exception as e:\n",
    "            log(f\"[interp-ocean] warning: time {t_val} failed: {e}\")\n",
    "            continue\n",
    "\n",
    "        row_ids_chunk = df_chunk[\"row_id\"].values\n",
    "        for v in vars_:\n",
    "            a = primary[v].values\n",
    "            m = np.isnan(a)\n",
    "            if m.any():\n",
    "                a[m] = nearest[v].values[m]\n",
    "            out.loc[row_ids_chunk, v] = a.astype(\"float32\")\n",
    "\n",
    "    out.index.name = \"row_id\"\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80746184",
   "metadata": {},
   "source": [
    "##  --- 4. LOOP OVER MONTHS: DOWNLOAD → INTERP → WRITE ---\n",
    " - For each month:\n",
    "   - Subset telemetry rows\n",
    "   - Compute month-specific bbox\n",
    "   - Download Copernicus subset\n",
    "   - Interpolate onto ship track\n",
    "   - Write monthly parquet: `ocean_interp_YYYY-MM.parquet`\n",
    " - Skip months already processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6166fe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def process_ocean_all_months(df_all: pd.DataFrame, months, vars_list, out_dir, tmp_dir, pad: float = 2.0):\n",
    "    \"\"\"\n",
    "    Loop over months, compute a tight bbox for EACH MONTH from telemetry,\n",
    "    download Copernicus subset, interpolate to track, and write parquet.\n",
    "    \"\"\"\n",
    "    out_parts = []\n",
    "\n",
    "    for ym in months:\n",
    "        ds = None\n",
    "        nc_path = os.path.join(tmp_dir, f\"OCEAN_{ym}.nc\")\n",
    "\n",
    "        try:\n",
    "            # Subset telemetry for this month\n",
    "            mask = df_all[\"ts_naive\"].dt.to_period(\"M\") == pd.Period(ym)\n",
    "            df_m = df_all.loc[mask, [\"row_id\", \"timestamp\", \"ts_naive\", \"Latitude_deg\", \"Longitude_deg\"]].copy()\n",
    "            if df_m.empty:\n",
    "                log(f\"[skip-ocean] {ym}: no rows in this month\")\n",
    "                continue\n",
    "\n",
    "            # Month-specific bbox\n",
    "            area_m = month_bbox(df_m, pad=pad)   # [N, W, S, E]\n",
    "            log(f\"[dl-ocean] {ym} bbox={area_m} → {nc_path}\")\n",
    "\n",
    "            # Download (or reuse)\n",
    "            nc_path = ocean_download_month(ym, area_m, vars_list, nc_path)\n",
    "\n",
    "            # Open and inspect\n",
    "            log(f\"[open-ocean] {ym}: {nc_path}\")\n",
    "            ds = open_ocean_nc(nc_path)\n",
    "            try:\n",
    "                log(f\"[info-ocean] {ym}: vars={list(ds.data_vars)[:8]} ...\")\n",
    "                log(f\"[info-ocean] {ym}: coords={list(ds.coords)}\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            # Interpolate\n",
    "            log(f\"[interp-ocean] {ym} for {len(df_m)} rows …\")\n",
    "            df_o = ocean_interp_time_space(ds, df_m, time_tol=\"90min\")\n",
    "\n",
    "            # Write parquet\n",
    "            part_path = os.path.join(out_dir, f\"ocean_interp_{ym}.parquet\")\n",
    "            df_o.to_parquet(part_path)\n",
    "            out_parts.append(part_path)\n",
    "            log(f\"[done-ocean] {ym}: wrote {part_path} (cols={list(df_o.columns)})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            log(f\"[ERROR-ocean] {ym}: {e}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "        finally:\n",
    "            if ds is not None:\n",
    "                try:\n",
    "                    ds.close()\n",
    "                except Exception:\n",
    "                    pass\n",
    "            # If you want, you can clean nc_path here to save disk.\n",
    "\n",
    "    return out_parts\n",
    "\n",
    "\n",
    "_done = {\n",
    "    os.path.basename(p)[len(\"ocean_interp_\"):-len(\".parquet\")]\n",
    "    for p in glob(os.path.join(OUT_DIR_OCEAN, \"ocean_interp_*.parquet\"))\n",
    "}\n",
    "months_todo = [m for m in months if m not in _done]\n",
    "log(f\"Remaining OCEAN months: {months_todo}\")\n",
    "\n",
    "ocean_parts = process_ocean_all_months(\n",
    "    df_all   = df_ocean,\n",
    "    months   = months_todo,\n",
    "    vars_list= OCEAN_VARS,\n",
    "    out_dir  = OUT_DIR_OCEAN,\n",
    "    tmp_dir  = OCEAN_TMP_DIR,\n",
    "    pad      = 2.0,  # same padding as global, but per-month\n",
    ")\n",
    "\n",
    "log(f\"Ocean monthly parts written: {len(ocean_parts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043e1eab",
   "metadata": {},
   "source": [
    "##  --- 5. MERGE OCEAN ONTO TELEMETRY+ERA5 ---\n",
    " - Concatenate all monthly ocean parquet files\n",
    " - Merge back onto original df via `row_id`\n",
    " - Save an intermediate parquet with ERA5 + ocean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356ddcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "part_files = sorted(glob(os.path.join(OUT_DIR_OCEAN, \"ocean_interp_*.parquet\")))\n",
    "if part_files:\n",
    "    ocean_all = pd.concat([pd.read_parquet(p) for p in part_files]).sort_index()\n",
    "    log(f\"ocean rows: {len(ocean_all)} | columns: {list(ocean_all.columns)}\")\n",
    "\n",
    "    df_full = df.merge(ocean_all, on=\"row_id\", how=\"left\")\n",
    "\n",
    "    FINAL_OCEAN = os.path.join(OUT_DIR_OCEAN, \"telemetry_with_era5_ocean.parquet\")\n",
    "    df_full.to_parquet(FINAL_OCEAN, index=False)\n",
    "    log(f\"saved: {FINAL_OCEAN}\")\n",
    "else:\n",
    "    log(\"No ocean parquet parts found; nothing merged.\")\n",
    "    df_full = None  # to avoid NameError later if nothing is merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c0b9a",
   "metadata": {},
   "source": [
    "##  --- 6. COMPUTE SURFACE DENSITY (GSW) & SAVE FINAL DATASET ---\n",
    " - Load the telemetry+ERA5+ocean file (or use df_full)\n",
    " - Use GSW to compute in-situ density at surface (p ≈ 0 dbar)\n",
    " - Save final dataset with `rho_surface`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2564239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gsw\n",
    "\n",
    "if df_full is None:\n",
    "    # if notebook resumed and df_full not in memory\n",
    "    FINAL_OCEAN = os.path.join(OUT_DIR_OCEAN, \"telemetry_with_era5_ocean.parquet\")\n",
    "    df_full = pd.read_parquet(FINAL_OCEAN)\n",
    "\n",
    "T = df_full[\"thetao\"].to_numpy()\n",
    "S = df_full[\"so\"].to_numpy()\n",
    "p = np.zeros_like(T)  # surface pressure ~ 0 dbar\n",
    "\n",
    "rho = gsw.rho(S, T, p)  # kg/m^3\n",
    "df_full[\"rho_surface\"] = rho\n",
    "\n",
    "FINAL_DENS = os.path.join(OUT_DIR_OCEAN, \"telemetry_with_era5_ocean_density.parquet\")\n",
    "df_full.to_parquet(FINAL_DENS, index=False)\n",
    "log(f\"saved: {FINAL_DENS} with rho_surface\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b433af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "final_path = \"test/telemetry_with_era5_ocean_density.parquet\"\n",
    "df_final = pd.read_parquet(final_path)\n",
    "\n",
    "print(\"Final shape:\", df_final.shape)\n",
    "\n",
    "print(df_final[[\n",
    "    \"timestamp\",\n",
    "    \"Latitude_deg\",\n",
    "    \"Longitude_deg\",\n",
    "    \"u10\", \"v10\",\n",
    "    \"uo\", \"vo\",\n",
    "    \"thetao\", \"so\",\n",
    "    \"rho_surface\"\n",
    "]].head())\n",
    "\n",
    "print(\"\\nNaN fractions:\")\n",
    "for c in [\"u10\", \"v10\", \"uo\", \"vo\", \"thetao\", \"so\", \"rho_surface\"]:\n",
    "    if c in df_final.columns:\n",
    "        print(f\"{c:12s}: {df_final[c].isna().mean():6.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "era5-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
